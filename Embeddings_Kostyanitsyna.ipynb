{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import Counter,defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('/Users/Stoneberry/Downloads/paraphraser/paraphrases.xml', 'rb').read())\n",
    "\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "2     0  Президент Египта ввел чрезвычайное положение в...   \n",
       "3    -1  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4     0  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2  \n",
       "0  Полиции могут разрешить стрелять по хулиганам ...  \n",
       "1  Правила внесудебного проникновения полицейских...  \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...  \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...  \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
       "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>право полицейский проникновение жилища решить ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>президент египет ввести чрезвычайный положение...</td>\n",
       "      <td>власть египет угрожать ввести страна чрезвычай...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>москва сирия вернуться 2 самолёт мчс россиянин...</td>\n",
       "      <td>самолёт мчс вывезти россиянин разрушить сирия</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             text_1  \\\n",
       "0     0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1     0  Право полицейских на проникновение в жилище ре...   \n",
       "2     0  Президент Египта ввел чрезвычайное положение в...   \n",
       "3    -1  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4     0  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...   \n",
       "1  Правила внесудебного проникновения полицейских...   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешить стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилища решить ...   \n",
       "2  президент египет ввести чрезвычайный положение...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолёт мчс россиянин...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция мочь разрешить стрелять хулиган травма...  \n",
       "1  правило внесудебный проникновение полицейский ...  \n",
       "2  власть египет угрожать ввести страна чрезвычай...  \n",
       "3      самолёт мчс вывезти россиянин разрушить сирия  \n",
       "4      самолёт мчс вывезти россиянин разрушить сирия  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тексты для train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('/Users/Stoneberry/Downloads/news_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>канцлер германия ангел меркель ход брифинг пре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>российский белорусский войско успешно завершит...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>дзюба шат анюк оказаться нужный зенит российск...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>испания фанат пожалуй главный пятничный новост...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>постпред россия оон виталий чуркин говорить ве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \n",
       "0  канцлер германия ангел меркель ход брифинг пре...  \n",
       "1  российский белорусский войско успешно завершит...  \n",
       "2  дзюба шат анюк оказаться нужный зенит российск...  \n",
       "3  испания фанат пожалуй главный пятничный новост...  \n",
       "4  постпред россия оон виталий чуркин говорить ве...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X_cv = cv.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X_tf = tfidf.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразуйте тексты в векторы в каждой паре 4 методами  - SVD, NMF, Word2Vec, Fastext. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для SVD и NMF сделайте две пары векторов - через TfidfVectorizer и CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_cv = TruncatedSVD(50)\n",
    "svd_cv.fit(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = svd_cv.transform(cv.transform(data['text_1_norm']))\n",
    "X_text_2 = svd_cv.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_svd_cv = [X_text_1, X_text_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tf = TruncatedSVD(50)\n",
    "svd_tf.fit(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1 = svd_tf.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2 = svd_tf.transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_svd_tf = [X_text_1, X_text_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_svd(svd_cv, svd_tf):\n",
    "    \n",
    "    global data, cv, tfidf\n",
    "\n",
    "    X_text_1 = svd_cv.transform(cv.transform(data['text_1_norm']))\n",
    "    X_text_2 = svd_cv.transform(cv.transform(data['text_2_norm']))\n",
    "    X_svd_cv = [X_text_1, X_text_2]\n",
    "    \n",
    "    X_text_1 = svd_tf.transform(tfidf.transform(data['text_1_norm']))\n",
    "    X_text_2 = svd_tf.transform(tfidf.transform(data['text_2_norm']))\n",
    "    X_svd_tf = [X_text_1, X_text_2]\n",
    "    \n",
    "    return [X_svd_cv, X_svd_tf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=50, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_cv = NMF(50)\n",
    "nmf_cv.fit(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_nmf = nmf_cv.transform(cv.transform(data['text_1_norm']))\n",
    "X_text_2_nmf = nmf_cv.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_nmf_cv = [X_text_1_nmf, X_text_2_nmf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=50, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_tf = NMF(50)\n",
    "nmf_tf.fit(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_nmf = nmf_tf.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_nmf = nmf_tf.transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_nmf_tf = [X_text_1_nmf, X_text_2_nmf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_NMF(nmf_cv, nmf_tf):\n",
    "    \n",
    "    global data, cv, tfidf\n",
    "    \n",
    "    X_text_1_nmf = nmf_cv.transform(cv.transform(data['text_1_norm']))\n",
    "    X_text_2_nmf = nmf_cv.transform(cv.transform(data['text_2_norm']))\n",
    "    X_text_nmf_cv = [X_text_1_nmf, X_text_2_nmf]\n",
    "\n",
    "    X_text_1_nmf = nmf_tf.transform(tfidf.transform(data['text_1_norm']))\n",
    "    X_text_2_nmf = nmf_tf.transform(tfidf.transform(data['text_2_norm']))  \n",
    "    X_text_nmf_tf = [X_text_1_nmf, X_text_2_nmf]\n",
    "    \n",
    "    return [X_text_nmf_cv, X_text_nmf_tf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim, tfidf=False):\n",
    "    \n",
    "    if tfidf:\n",
    "        vocab = tfidf.vocabulary_\n",
    "        arr = tfidf.transform([' '.join(text)]).toarray()[0]\n",
    "    \n",
    "    text = text.split()\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        \n",
    "        try:\n",
    "            if tfidf and word in vocab:\n",
    "                weight = arr[vocab[word]]\n",
    "            else:\n",
    "                weight = words[word]/total\n",
    "\n",
    "            v = model[word]\n",
    "            vectors[i] = v * weight\n",
    "\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)\n",
    "\n",
    "X_text_w2v = [X_text_1_w2v, X_text_2_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim, tfidf)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim, tfidf)\n",
    "\n",
    "X_text_w2v_tf = [X_text_1_w2v, X_text_2_w2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_w2v(w2v, dim):\n",
    "    \n",
    "    global data, tfidf\n",
    "\n",
    "    X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "    X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "    for i, text in enumerate(data['text_1_norm'].values):\n",
    "        X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "            \n",
    "    for i, text in enumerate(data['text_2_norm'].values):\n",
    "        X_text_2_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    X_text_w2v = [X_text_1_w2v, X_text_2_w2v]\n",
    "    \n",
    "\n",
    "    X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "    X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "    for i, text in enumerate(data['text_1_norm'].values):\n",
    "        X_text_1_w2v[i] = get_embedding(text, w2v, dim, tfidf)\n",
    "    \n",
    "    for i, text in enumerate(data['text_2_norm'].values):\n",
    "        X_text_2_w2v[i] = get_embedding(text, w2v, dim, tfidf)\n",
    "    X_text_w2v_tf = [X_text_1_w2v, X_text_2_w2v]\n",
    "    \n",
    "    return [X_text_w2v, X_text_w2v_tf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fastext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "без нормализации и с нормализацией, а через каждую модель постройте две пары векторов -  с взвешиванием по tfidf и без. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_norm = gensim.models.FastText([text.split() for text in data_rt['content_norm']], size=50, min_n=4, max_n=8)\n",
    "fast_text_nenorm = gensim.models.FastText([text.split() for text in data_rt['content']], size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_notnorm'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_notnorm'] = data['text_2'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text_nenorm, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text_nenorm, dim)\n",
    "\n",
    "X_text_ft_ne = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text_nenorm, dim, tfidf)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text_nenorm, dim, tfidf)\n",
    "\n",
    "X_text_ft_ne_tf = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text_norm, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text_norm, dim)\n",
    "\n",
    "X_text_ft_norm = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text_norm, dim, tfidf)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text_norm, dim, tfidf)\n",
    "\n",
    "X_text_ft_norm_tf = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast(fast_text_norm, fast_text_nenorm, dim):\n",
    "    \n",
    "    global data\n",
    "\n",
    "    def foo(data, dim, name1, name2, fast_text_norm, fast_text_nenorm, tf=False):\n",
    "\n",
    "        X_text_1_ft = np.zeros((len(data[name1]), dim))\n",
    "        X_text_2_ft = np.zeros((len(data[name2]), dim))\n",
    "\n",
    "        for i, text in enumerate(data[name1].values):\n",
    "            if not tf:\n",
    "                X_text_1_ft[i] = get_embedding(text, fast_text_nenorm, dim)\n",
    "            else:\n",
    "                X_text_1_ft[i] = get_embedding(text, fast_text_norm, dim, tfidf)\n",
    "    \n",
    "        for i, text in enumerate(data[name2].values):\n",
    "            if not tf:\n",
    "                X_text_2_ft[i] = get_embedding(text, fast_text_nenorm, dim)\n",
    "            else:\n",
    "                X_text_2_ft[i] = get_embedding(text, fast_text_norm, dim, tfidf)\n",
    "\n",
    "        return [X_text_1_ft, X_text_2_ft]\n",
    "    \n",
    "    return [foo(data, dim, 'text_1_notnorm', 'text_2_notnorm', fast_text_norm, fast_text_nenorm, tf=False), \n",
    "            foo(data, dim, 'text_1_norm', 'text_2_norm', fast_text_norm, fast_text_nenorm, tf=False), \n",
    "            foo(data, dim, 'text_1_notnorm', 'text_2_notnorm', fast_text_norm, fast_text_nenorm, tf=True),\n",
    "            foo(data, dim, 'text_1_norm', 'text_2_norm', fast_text_norm, fast_text_nenorm, tf=True)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Между векторами каждой пары вычислите косинусную близость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_together = [X_text_ft_norm_tf, X_text_ft_norm, X_text_ft_ne_tf, X_text_ft_ne,\n",
    "               X_text_w2v_tf, X_text_w2v, X_text_nmf_tf, X_text_nmf_cv,\n",
    "               X_svd_tf, X_svd_cv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for index, pair in enumerate(all_together):\n",
    "    x = pair[0]\n",
    "    y = pair[1]\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        ans = cosine_distances(x[i].reshape(1, -1), y[i].reshape(1, -1))[0]\n",
    "        res.append(ans[0])\n",
    "    results[index] = res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.278050</td>\n",
       "      <td>0.263309</td>\n",
       "      <td>0.129570</td>\n",
       "      <td>0.087815</td>\n",
       "      <td>0.095309</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.574292</td>\n",
       "      <td>0.474685</td>\n",
       "      <td>0.793834</td>\n",
       "      <td>0.823382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.092989</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.101363</td>\n",
       "      <td>0.104367</td>\n",
       "      <td>0.054607</td>\n",
       "      <td>0.084820</td>\n",
       "      <td>0.192967</td>\n",
       "      <td>0.596678</td>\n",
       "      <td>0.551834</td>\n",
       "      <td>0.584042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364554</td>\n",
       "      <td>0.162135</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.045786</td>\n",
       "      <td>0.076380</td>\n",
       "      <td>0.042211</td>\n",
       "      <td>0.320273</td>\n",
       "      <td>0.916755</td>\n",
       "      <td>0.580027</td>\n",
       "      <td>0.834902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.849245</td>\n",
       "      <td>0.386477</td>\n",
       "      <td>0.531361</td>\n",
       "      <td>0.168232</td>\n",
       "      <td>0.472830</td>\n",
       "      <td>0.281317</td>\n",
       "      <td>0.672072</td>\n",
       "      <td>0.347814</td>\n",
       "      <td>0.407976</td>\n",
       "      <td>0.309526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.908951</td>\n",
       "      <td>0.295414</td>\n",
       "      <td>0.774327</td>\n",
       "      <td>0.475915</td>\n",
       "      <td>0.271393</td>\n",
       "      <td>0.084477</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.123715</td>\n",
       "      <td>0.016991</td>\n",
       "      <td>0.072903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.278050  0.263309  0.129570  0.087815  0.095309  0.077762  0.574292   \n",
       "1  0.092989  0.188930  0.101363  0.104367  0.054607  0.084820  0.192967   \n",
       "2  0.364554  0.162135  0.031763  0.045786  0.076380  0.042211  0.320273   \n",
       "3  0.849245  0.386477  0.531361  0.168232  0.472830  0.281317  0.672072   \n",
       "4  0.908951  0.295414  0.774327  0.475915  0.271393  0.084477  0.002551   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.474685  0.793834  0.823382  \n",
       "1  0.596678  0.551834  0.584042  \n",
       "2  0.916755  0.580027  0.834902  \n",
       "3  0.347814  0.407976  0.309526  \n",
       "4  0.123715  0.016991  0.072903  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(results)\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постройте обучающую выборку из этих близостей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(table, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучите любую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'C':[1, 10, 100, 100]}\n",
    "est = LogisticRegression(class_weight='balanced', random_state=42, multi_class='auto')\n",
    "clf = GridSearchCV(est, params, cv=3)\n",
    "clf.fit(train_X, train_y)\n",
    "clf.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 0.5379081350304372\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, class_weight='balanced',  multi_class='auto', random_state=42)\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print('test', f1_score(valid_y, preds, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оцените качество на кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5531783025076636"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, class_weight='balanced', multi_class='auto', random_state=42)\n",
    "np.mean(cross_val_score(clf, table, y, cv=5, scoring=make_scorer(f1_score, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С помощью кросс-валидации подберите параметры моделей (количество компонент, размерность в w2v, min_n - в fastext и т.д)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(10, 50, 1, 2), (50, 50, 1, 2), (100, 50, 1, 2), \n",
    "          (10, 100, 2, 3), (50, 100, 2, 3), (100, 100, 2, 3),]\n",
    "          #(10, 300, 4, 5), (50, 300, 4, 5), (100, 300, 4, 5)\n",
    "         #]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(param, X_cv, X_tf, data_rt, y, C=10):\n",
    "\n",
    "    all_together = []\n",
    "    \n",
    "    svd_cv = TruncatedSVD(param[0])\n",
    "    svd_cv.fit(X_cv)\n",
    "    svd_tf = TruncatedSVD(param[0])\n",
    "    svd_tf.fit(X_tf)\n",
    "    all_together += my_svd(svd_cv, svd_tf)\n",
    "\n",
    "    nmf_cv = NMF(param[0])\n",
    "    nmf_cv.fit(X_cv)\n",
    "    nmf_tf = NMF(param[0])\n",
    "    nmf_tf.fit(X_tf)\n",
    "    all_together += my_NMF(nmf_cv, nmf_tf)\n",
    "    \n",
    "    dim = param[1]\n",
    "\n",
    "    w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=param[1], sg=1)\n",
    "    all_together += my_w2v(w2v, dim)\n",
    "\n",
    "    fast_text_norm = gensim.models.FastText([text.split() for text in data_rt['content_norm']], \n",
    "                        size=param[1], min_n=param[2], max_n=param[3])\n",
    "    fast_text_nenorm = gensim.models.FastText([text.split() for text in data_rt['content']], \n",
    "                          size=param[1], min_n=param[2], max_n=param[3])\n",
    "    all_together += fast(fast_text_norm, fast_text_nenorm, dim)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    for index, pair in enumerate(all_together):\n",
    "        x = pair[0]\n",
    "        z = pair[1]\n",
    "        res = []\n",
    "        for i in range(len(x)):\n",
    "            ans = cosine_distances(x[i].reshape(1, -1), z[i].reshape(1, -1))[0]\n",
    "            res.append(ans[0])\n",
    "        results[index] = res\n",
    "    \n",
    "    table = pd.DataFrame(results)\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(table, y, random_state=42)\n",
    "    \n",
    "    clf = LogisticRegression(C=C, class_weight='balanced', multi_class='auto', random_state=42)\n",
    "    return [train_X, valid_X, train_y, valid_y], np.mean(cross_val_score(clf, table, y, cv=3,\n",
    "                         scoring=make_scorer(f1_score, average='micro')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(params, X_cv, X_tf, data_rt, y):\n",
    "    \n",
    "    ans = {}\n",
    "    t_par = tqdm_notebook(params, desc='params', leave=True)\n",
    "    \n",
    "    for param in t_par:\n",
    "        \n",
    "        ans[param] = fitting(param, X_cv, X_tf, data_rt, y)[1]\n",
    "\n",
    "    return sorted(ans.items(), key=lambda kv: kv[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b4e2deb3c488a9d62233509cebfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='params', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((10, 100, 2, 3), 0.45703361516114027),\n",
       " ((100, 100, 2, 3), 0.4676874736451764),\n",
       " ((50, 100, 2, 3), 0.47045426104559096),\n",
       " ((50, 50, 1, 2), 0.5577667100495336),\n",
       " ((100, 50, 1, 2), 0.5579048311522172),\n",
       " ((10, 50, 1, 2), 0.5630246936177559)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid(params, X_cv, X_tf, data_rt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3358bddc5f6a47999fffcf41b62447d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='params', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((10, 300, 4, 5), 0.45758705688475687),\n",
       " ((50, 300, 4, 5), 0.46893240161006244),\n",
       " ((100, 300, 4, 5), 0.46948473285073533)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid([(10, 300, 4, 5), (50, 300, 4, 5), (100, 300, 4, 5)], X_cv, X_tf, data_rt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = fitting((10, 50, 1, 2), X_cv, X_tf, data_rt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting(param, X_cv, X_tf, data_rt, y, C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'C':[0.1, 1, 10, 100, 1000], 'solver':['lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "est = LogisticRegression(class_weight='balanced', random_state=42, multi_class='auto')\n",
    "clf = GridSearchCV(est, params, cv=3)\n",
    "clf.fit(dt[0][0], dt[0][2])\n",
    "clf.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(10, 50, 1, 5), (50, 50, 1, 5), (5, 50, 1, 5), \n",
    "          (10, 10, 2, 3), (50, 10, 2, 3), (5, 10, 2, 3),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580e045d9ff94687bc0cf01402f93093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='params', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((50, 10, 2, 3), 0.5430763546387581),\n",
       " ((5, 10, 2, 3), 0.5448780384091811),\n",
       " ((10, 10, 2, 3), 0.5451555255217464),\n",
       " ((50, 50, 1, 5), 0.5652229177117685),\n",
       " ((5, 50, 1, 5), 0.5654976466173752),\n",
       " ((10, 50, 1, 5), 0.5670209813355582)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid(params, X_cv, X_tf, data_rt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb83694bf3246468b3188244346c799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='params', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((100, 10, 2, 5), 0.543219838863251),\n",
       " ((50, 10, 2, 5), 0.5454312887549626),\n",
       " ((200, 10, 2, 5), 0.5459841943248742),\n",
       " ((100, 50, 1, 8), 0.5603781846516126),\n",
       " ((200, 50, 1, 8), 0.5605171867897976),\n",
       " ((50, 50, 1, 8), 0.5639777596578903)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [(100, 50, 1, 8), (50, 50, 1, 8), (200, 50, 1, 8), \n",
    "          (100, 10, 2, 5), (50, 10, 2, 5), (200, 10, 2, 5),]\n",
    "\n",
    "grid(params, X_cv, X_tf, data_rt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c766b3b8157a4cd0aba4040351e977a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='params', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[((10, 50, 1, 8), 0.568405831079361)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = [(10, 50, 1, 8)]\n",
    "\n",
    "grid(params, X_cv, X_tf, data_rt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = fitting((10, 50, 1, 8), X_cv, X_tf, data_rt, y)\n",
    "\n",
    "\n",
    "params = {'C':[0.1, 1, 10, 100, 1000], 'solver':['lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "est = LogisticRegression(class_weight='balanced', random_state=42, multi_class='auto')\n",
    "clf = GridSearchCV(est, params, cv=3)\n",
    "clf.fit(dt[0][0], dt[0][2])\n",
    "clf.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([dt[0][0], dt[0][1]])\n",
    "y_new = np.concatenate((dt[0][2], dt[0][3]), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5675911256989612"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1, class_weight='balanced', multi_class='auto', random_state=42)\n",
    "np.mean(cross_val_score(clf, result, y_new, cv=3, scoring=make_scorer(f1_score, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666851134477033"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = LogisticRegression(class_weight='balanced', random_state=42, multi_class='auto')\n",
    "est.fit(dt[0][0], dt[0][2])\n",
    "pred = est.predict(dt[0][1])\n",
    "f1_score(dt[0][3], pred, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
